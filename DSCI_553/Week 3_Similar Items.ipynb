{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='review/sample.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(tokens,n_grams):\n",
    "    return zip(*[tokens[i:] for i in range(n_grams)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash_func():\n",
    "    a = np.random.randint(-100, 100)\n",
    "    b = np.random.randint(-100, 100)\n",
    "    c = 4294967311\n",
    "    def hash_func(x):\n",
    "        return (a*x+b) % c\n",
    "    return hash_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingling(filename, n_grams, hash_size):\n",
    "    lines=len([1 for line in open(filename,'r',encoding='utf-8')])\n",
    "    matrix=scipy.sparse.lil_matrix((hash_size,lines),dtype=bool)\n",
    "    \n",
    "    for i,line in enumerate(open(filename,'r',encoding='utf-8')):\n",
    "        tokens=[t for t in line[:-1].split(' ') if t!='']\n",
    "        tokens=get_ngrams(tokens,n_grams)\n",
    "        for token in tokens:\n",
    "            bucket=hash(token)%hash_size\n",
    "            matrix[bucket,i]=1\n",
    "            \n",
    "    matrix=scipy.sparse.csc_matrix(matrix)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minhashing(matrix, hash_funcs):\n",
    "    k=len(hash_funcs)\n",
    "    n_cols=matrix.shape[1]\n",
    "    M=np.zeros((k,n_cols))\n",
    "    for i in range(n_cols):\n",
    "        rows=matrix[:,i].indices\n",
    "        for j,hash_func in enumerate(hash_funcs):\n",
    "            vhash_list=[hash_func(v) for v in rows]\n",
    "            vhash_min=min(vhash_list)\n",
    "            M[j,i]=vhash_min\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSH(M,b=20,r=5):\n",
    "    n_cols=M.shape[1]\n",
    "    k=n_cols\n",
    "    bucket_list=[[[] for i in range(k)] for j in range(b)]\n",
    "    for c in range(n_cols):\n",
    "        for band in range(b):\n",
    "            row_start=band*r\n",
    "            bucket=tuple(M[row_start:(row_start+r),c])\n",
    "            vbucket=hash(bucket)%k\n",
    "            bucket_list[band][vbucket].append(c)\n",
    "    return bucket_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(bucket_list, query=None):\n",
    "    candidates=set()\n",
    "    count=0\n",
    "    for band in bucket_list:\n",
    "        for vbucket in band:\n",
    "            if query!=None:\n",
    "                if query in vbucket:\n",
    "                    candidates=candidates.union(set(vbucket))\n",
    "            else:\n",
    "                candidates=candidates.union(list(itertools.combinations(vbucket,2)))\n",
    "        count+=1\n",
    "#         print(f\"Band {count} completed, {len(candidates)} candidate pairs found\")\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_item(bucket_list, matrix, query, SIM=0.8):\n",
    "    candidates=get_candidates(bucket_list,query)\n",
    "    \n",
    "    sims=[]\n",
    "    c1=set(matrix[:,query].indices)\n",
    "    for candidate in candidates:\n",
    "        c2=set(matrix[:,candidate].indices)\n",
    "        sim=len(c1 & c2)/len(c1 | c2)\n",
    "        if sim>=SIM:\n",
    "            sims.append((candidate,sim))\n",
    "    sims=sorted(sims,key=lambda x:x[1],reverse=True)           \n",
    "    \n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(467, 1.0), (574, 0.5673076923076923), (466, 0.5673076923076923), (603, 0.5673076923076923)]\n",
      "No Spark solution time elapsed: 3 minutes 9 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "\n",
    "k=100\n",
    "# find and hash n-grams, create sparse matrix\n",
    "n_grams=5\n",
    "hash_size=2**16\n",
    "matrix=shingling(filename, n_grams, hash_size)\n",
    "\n",
    "# create hash functions\n",
    "hash_funcs=[get_hash_func() for i in range(k)]\n",
    "\n",
    "# minhashing and create M\n",
    "M=minhashing(matrix, hash_funcs)\n",
    "\n",
    "# LSH and create bucket_list\n",
    "bucket_list=LSH(M,b=20,r=5)\n",
    "\n",
    "# find similar pairs\n",
    "sims=find_similar_item(bucket_list, matrix, 467, 0.5)\n",
    "\n",
    "print(sims)\n",
    "\n",
    "end_time=time.time()\n",
    "elapsed=end_time-start_time\n",
    "print(f\"No Spark solution time elapsed: {int(elapsed//60)} minutes {int(elapsed%60)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Spark Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    sc=SparkContext(\"local[*]\",\"Similar Items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim(pair,matrix):\n",
    "    i1,i2=pair\n",
    "    c1=set(matrix[:,i1].indices)\n",
    "    c2=set(matrix[:,i2].indices)\n",
    "    sim=len(c1 & c2)/len(c1 | c2)\n",
    "    return (pair, sim)\n",
    "\n",
    "def get_sim_other(item,query,matrix):\n",
    "    c1=set(matrix[:,query].indices)\n",
    "    c2=set(matrix[:,item].indices)\n",
    "    sim=len(c1 & c2)/len(c1 | c2)\n",
    "    return (item, sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(467, 1.0), (466, 0.5673076923076923), (603, 0.5673076923076923), (574, 0.5673076923076923)]\n",
      "[((2621, 2656), 1.0), ((8523, 8702), 1.0), ((2943, 2946), 1.0), ((5934, 5959), 0.9318181818181818), ((5958, 5959), 0.9318181818181818), ((2902, 2922), 0.9170731707317074), ((5023, 9620), 0.8461538461538461), ((4928, 4929), 0.8451327433628318), ((2636, 2646), 0.810126582278481), ((2646, 2647), 0.810126582278481)]\n",
      "Partial Spark solution time elapsed: 3 minutes 26 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "\n",
    "k=100\n",
    "n_grams=5\n",
    "hash_size=2**16\n",
    "hash_funcs=[get_hash_func() for i in range(k)]\n",
    "\n",
    "# find and hash n-grams, create sparse matrix\n",
    "matrix=shingling(filename, n_grams, hash_size)\n",
    "\n",
    "# minhashing and create M\n",
    "M=minhashing(matrix, hash_funcs)\n",
    "\n",
    "# LSH and create bucket_list\n",
    "bucket_list=LSH(M,b=20,r=5)\n",
    "    \n",
    "# Given a query, get similar item\n",
    "query=467\n",
    "SIM=0.5\n",
    "sims=sc.parallelize(bucket_list).flatMap(lambda x:[i for i in x if query in i]).\\\n",
    "flatMap(lambda x:[i for i in x]).distinct().\\\n",
    "map(lambda x: get_sim_other(x,query,matrix)).filter(lambda x:x[1]>SIM).sortBy(lambda x:x[1],False).collect()\n",
    "\n",
    "print(sims)\n",
    "\n",
    "# Get all similar pairs\n",
    "SIM=0.8\n",
    "pairs=sc.parallelize(bucket_list).flatMap(lambda x:[i for i in x if len(i)>1]).\\\n",
    "flatMap(lambda x:list(itertools.combinations(x,2))).distinct().\\\n",
    "map(lambda x: get_sim(x,matrix)).filter(lambda x:x[1]>SIM).sortBy(lambda x:x[1],False).collect()\n",
    "\n",
    "print(pairs[-10:])\n",
    "\n",
    "end_time=time.time()\n",
    "elapsed=end_time-start_time\n",
    "print(f\"Partial Spark solution time elapsed: {int(elapsed//60)} minutes {int(elapsed%60)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Spark Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingling_spark(line, n_grams, hash_size=2**16):\n",
    "    mtx=scipy.sparse.lil_matrix((hash_size,1))\n",
    "    tokens=[t for t in line[:-1].split(' ') if t!='']\n",
    "    tokens=get_ngrams(tokens,n_grams)\n",
    "    for token in tokens:\n",
    "        h=hash(token)%hash_size\n",
    "        mtx[h,0]=1\n",
    "    mtx=scipy.sparse.csc_matrix(mtx)\n",
    "    return mtx\n",
    "\n",
    "def minhashing_spark(matrix, hash_funcs):\n",
    "    k=len(hash_funcs)\n",
    "    M=[0 for i in range(k)]\n",
    "    rows=matrix.indices\n",
    "    for j,hash_func in enumerate(hash_funcs):\n",
    "        vhash_list=[hash_func(v) for v in rows]\n",
    "        vhash_min=min(vhash_list)\n",
    "        M[j]=vhash_min\n",
    "    return M\n",
    "\n",
    "def LSH_spark(col,sig,b,r):\n",
    "    bucket_list=[]\n",
    "    k=2**16\n",
    "    for band in range(b):\n",
    "        start=band*r\n",
    "        bucket=hash(tuple(sig[start:(start+r)]))%k\n",
    "        bucket_list.append([(band,bucket),col])\n",
    "    return bucket_list\n",
    "\n",
    "def get_sim_other_spark(item,query,matrix):\n",
    "    c1=set(matrix[query].indices)\n",
    "    c2=set(matrix[item].indices)\n",
    "    sim=len(c1 & c2)/len(c1 | c2)\n",
    "    return (item, sim)\n",
    "\n",
    "def get_sim_spark(pair,matrix):\n",
    "    i1,i2=pair\n",
    "    c1=set(matrix[i1].indices)\n",
    "    c2=set(matrix[i2].indices)\n",
    "    sim=len(c1 & c2)/len(c1 | c2)\n",
    "    return (pair, sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(467, 1.0), (466, 0.5623003194888179), (603, 0.5623003194888179), (574, 0.5623003194888179)]\n",
      "[((2941, 2947), 1.0), ((5927, 5929), 1.0), ((5934, 5958), 1.0), ((2943, 2946), 1.0), ((2621, 2656), 1.0), ((8523, 8702), 1.0), ((5023, 9620), 0.8461538461538461), ((4928, 4929), 0.8451327433628318), ((2636, 2646), 0.8269230769230769), ((2646, 2647), 0.8269230769230769)]\n",
      "Full Spark solution time elapsed: 4 minutes 57 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "  \n",
    "n_grams=5\n",
    "hash_size=2**16\n",
    "hash_funcs=[get_hash_func() for i in range(k)]\n",
    "\n",
    "# Get n_grams and hash\n",
    "matrix=sc.textFile(filename).map(lambda x: shingling_spark(x, n_grams, hash_size)).collect()\n",
    "\n",
    "# Minhash\n",
    "M=sc.parallelize(matrix).map(lambda x: minhashing_spark(x, hash_funcs)).collect()\n",
    "MM=[(col,sig) for col,sig in enumerate(M)]\n",
    "\n",
    "# LSH\n",
    "b=20\n",
    "r=5\n",
    "bucket_list=sc.parallelize(MM).flatMap(lambda x: LSH_spark(x[0],x[1],b,r)).groupByKey().\\\n",
    "filter(lambda x: len(x[1])>1).map(lambda x: list(x[1])).collect()\n",
    "\n",
    "# Given a query, get all similar items\n",
    "query=467\n",
    "SIM=0.5\n",
    "sims=sc.parallelize(bucket_list).filter(lambda x: query in x).\\\n",
    "flatMap(lambda x:[i for i in x]).distinct().\\\n",
    "map(lambda x: get_sim_other_spark(x,query,matrix)).\\\n",
    "filter(lambda x:x[1]>SIM).sortBy(lambda x:x[1],False).collect()\n",
    "\n",
    "print(sims)\n",
    "\n",
    "# Get all similar pairs\n",
    "SIM=0.8\n",
    "pairs=sc.parallelize(bucket_list).\\\n",
    "flatMap(lambda x:list(itertools.combinations(x,2))).distinct().\\\n",
    "map(lambda x: get_sim_spark(x,matrix)).filter(lambda x:x[1]>SIM).sortBy(lambda x:x[1],False).collect()\n",
    "\n",
    "print(pairs[-10:])\n",
    "\n",
    "end_time=time.time()\n",
    "elapsed=end_time-start_time\n",
    "print(f\"Full Spark solution time elapsed: {int(elapsed//60)} minutes {int(elapsed%60)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #takes forever to run\n",
    "# def find_similar_pairs(bucket_list, matrix, SIM=0.8):\n",
    "#     candidates=get_candidates(bucket_list)\n",
    "#     print(\"Pairs of candidates:\", len(candidates))\n",
    "    \n",
    "#     similar_list=[]\n",
    "#     for candidate in candidates:\n",
    "#         i1,i2=candidate\n",
    "#         c1=set(matrix[:,i1].indices)\n",
    "#         c2=set(matrix[:,i2].indices)\n",
    "#         sim=len(c1 & c2)/len(c1 | c2)\n",
    "#         if sim>=SIM:\n",
    "#             similar_list.append((candidate,sim))\n",
    "                    \n",
    "#     return similar_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3_6]",
   "language": "python",
   "name": "conda-env-Python3_6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
